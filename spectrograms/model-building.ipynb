{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will be working with tensorflow to create models with the podcast audio data created in previous scripts and notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "podcast_audio_files = glob.glob('./audio-split-data/*.mp3', recursive=True)\n",
    "polly_audio_files = glob.glob('./polly-data/*.mp3', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = polly_audio_files.copy()\n",
    "ordered_pa = sorted(pa)\n",
    "\n",
    "pod_a = podcast_audio_files.copy()\n",
    "ordered_pod = sorted(pod_a)\n",
    "#grouping the audio file together by word\n",
    "grouped_audio = list(zip(ordered_pa, ordered_pod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading both the AWS polly generated audio and the actual episode audio chunks into librosa\n",
    "import numpy as np\n",
    "import librosa\n",
    "polly_arr = []\n",
    "episode_arr = []\n",
    "for group in grouped_audio[:10]:\n",
    "    polly = group[0]\n",
    "    episode_chunks = group[1]\n",
    "    episode_id = polly.split('/')[-1].split('.')[0]\n",
    "    try:\n",
    "        y_pol, sr_pol = librosa.load(polly, sr=11025)\n",
    "        y_episode, sr_episode = librosa.load(episode_chunks)\n",
    "        if (len(y_pol) <= 7000) and (len(y_episode) <= 15000):\n",
    "            polly_arr.append( y_pol)\n",
    "            episode_arr.append(y_episode)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "polly_arr = np.array(polly_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polly_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_arr = np.array(episode_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "       -2.6498399e-05,  2.4322753e-05,  3.3664037e-05], dtype=float32),\n",
       "       array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        3.0489087e-05, -2.5245014e-05, -7.3413677e-05], dtype=float32),\n",
       "       array([ 0.        ,  0.        ,  0.        , ...,  0.00011415,\n",
       "       -0.00028309, -0.00024886], dtype=float32),\n",
       "       array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        9.1746231e-05,  2.3154249e-05, -6.6433429e-05], dtype=float32),\n",
       "       array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        1.0346445e-04, -2.0771184e-04,  6.7976071e-05], dtype=float32),\n",
       "       array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "       -1.5773579e-05,  1.3382194e-04, -7.5663374e-05], dtype=float32),\n",
       "       array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "       -2.3723393e-10,  2.4218669e-10, -2.2643941e-10], dtype=float32),\n",
       "       array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        1.3683556e-05, -8.8883869e-05, -1.9079906e-05], dtype=float32),\n",
       "       array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        3.9846086e-06, -1.3105088e-06,  5.1963107e-05], dtype=float32),\n",
       "       array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        6.5245084e-05,  8.9974455e-05, -1.5754262e-06], dtype=float32)],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polly_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "polly_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    polly_arr, maxlen=7000, dtype='float32', padding='post', truncating='post',\n",
    "    value=0.0\n",
    ").reshape((10, 7000))\n",
    "# polly_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#     polly_arr, maxlen=7000, dtype='float32', padding='post', truncating='post',\n",
    "#     value=0.0\n",
    "# ).reshape((39933, 7000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polly_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('polly_seq.npy', polly_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "(10, 7000)\n"
     ]
    }
   ],
   "source": [
    "print(polly_seq.size)\n",
    "print(polly_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    episode_arr, maxlen=15000, dtype='float32', padding='post', truncating='post',\n",
    "    value=0.0\n",
    ").reshape((10, 15000,1))\n",
    "# episode_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#     episode_arr, maxlen=15000, dtype='float32', padding='post', truncating='post',\n",
    "#     value=0.0\n",
    "# ).reshape((39933, 15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "(10, 15000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(episode_seq.size)\n",
    "print(episode_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('episode_seq.npy', episode_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will map the episode chunks to the polly generated words. In other words, a group of episode chunk sequences will be mapped to one element in the polly generated words array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 1)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "#creating samples of data\n",
    "first_epi_sample = episode_seq[0,:,]\n",
    "first_polly_sample = polly_seq[0,:,]\n",
    "print(first_epi_sample.shape)\n",
    "print(first_polly_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_epi_to_polly(epi_sample, polly_sample, window):\n",
    "    \"\"\"A function to map the episode splices\n",
    "    to the polly generated words \n",
    "    for example: epi_sample = [1, 2, 3, 4, 5]\n",
    "    polly_sample =                  [6, 7, 8]\n",
    "    6 -->1, 2, 3\n",
    "    7 --> 2, 3, 4\n",
    "    8 --> 3, 4, 5\"\"\"\n",
    "    mapped_vec  = []\n",
    "    for i, sample in enumerate(polly_sample):\n",
    "        s = epi_sample[i:i+window]\n",
    "        mapped_vec.append([s, sample])\n",
    "    return mapped_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 3000, 1)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "# separating the mapped vec into x and y values\n",
    "test = map_epi_to_polly(first_epi_sample, first_polly_sample, 3000)\n",
    "X = [t[0] for t in test]\n",
    "y = [t[1] for t in test]\n",
    "X = np.array(X).reshape((7000, 3000, 1))\n",
    "y = np.array(y)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 3000, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq_len = [epi_seq.size==15000 for epi_seq in X]\n",
    "y_seq_len = [seq.size==7000 for seq in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure that every sequence in polly_seq is the proper length\n",
    "# pol_seq_len = [len(seq)==7000 for seq in polly_seq]\n",
    "# assert all(pol_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing the same test for episode sequences\n",
    "# epi_seq_len = [len(seqs)==15000 for seqs in episode_seq]\n",
    "# assert all(epi_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries to create deep learning model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "epi_input_layer = tf.keras.layers.InputLayer(\n",
    "    input_shape=(3000,1))\n",
    "model.add(epi_input_layer)\n",
    "model.add(layers.LSTM(128,))\n",
    "polly_output_layer = tf.keras.layers.Dense(\n",
    "    1, activation='relu')\n",
    "model.add(polly_output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               66560     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 66,689\n",
      "Trainable params: 66,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polly_seq.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "219/219 [==============================] - 300s 1s/step - loss: 0.0018\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 283s 1s/step - loss: 0.0018\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 273s 1s/step - loss: 0.0018\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 275s 1s/step - loss: 0.0018\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 274s 1s/step - loss: 0.0018\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 273s 1s/step - loss: 0.0018\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 296s 1s/step - loss: 0.0018\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 356s 2s/step - loss: 0.0018\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 406s 2s/step - loss: 0.0018\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 383s 2s/step - loss: 0.0018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fef5842f2e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X, y=y, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model trained above, the loss only went down between the first and second epochs. After that the loss remains the same. Perhaps, I need to run more epochs or change the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to run more epochs \n",
    "# maybe use more data because the model is not doing well right now\n",
    "# more research to find ways to improve model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, the model is not performing very well. It's not at all accurate and the loss is the same throughout the epochs. I will do more research on how to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epi_train, epi_test, pol_train, pol_test = train_test_split(episode_seq, polly_seq, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running a different architecture\n",
    "model_2 = keras.Sequential()\n",
    "epi_input_layer = tf.keras.layers.InputLayer(\n",
    "    input_shape=(3000,1))\n",
    "model_2.add(epi_input_layer)\n",
    "model_2.add(layers.LSTM(128,))\n",
    "model_2.add(layers.Dense(32, activation='relu'))\n",
    "polly_output_layer = tf.keras.layers.Dense(\n",
    "    1, activation='relu')\n",
    "model_2.add(polly_output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(\n",
    "    optimizer='adam', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_2.fit(x=X, y=y, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MFCCs to build an RNN\n",
    "for array in polly_arr[:10]:\n",
    "    polly_mfcc = librosa.feature.mfcc(array, sr=sr_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epi_splice in episode_arr[:10]:\n",
    "    episode_mfcc = librosa.feature.mfcc(epi_splice, sr=sr_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating model\n",
    "model_2 = keras.Sequential()\n",
    "epi_input = keras.layers.InputLayer(input_shape=(15000, 1))\n",
    "model_2.add(epi_input)\n",
    "model_2.add(layers.LSTM(128))\n",
    "polly_output = tf.keras.layers.Dense(\n",
    "    7000, activation='relu')\n",
    "model_2.add(polly_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(\n",
    "    optimizer='adam', loss='mse', metrics='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_2.fit(x=episode_seq, y=polly_seq, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
